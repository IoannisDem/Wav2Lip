{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "import audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "\n",
    "###################\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hparams import hparams as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Code to train the expert lip-sync discriminator')\n",
    "\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed LRS2 dataset\", required=True)\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)\n",
    "parser.add_argument('--checkpoint_path', help='Resumed from this checkpoint', default=None, type=str)\n",
    "\n",
    "data_root = \"C:/Users/ioann/Desktop/projects/Github/Wav2Lip/data/lrs2_preprocessed\"\n",
    "checkpoint_dir = \"C:/Users/ioann/Desktop/projects/Github/Wav2Lip/checkpoints\"\n",
    "checkpoint_path = \"\"\n",
    "\n",
    "# Parse the arguments? with the values\n",
    "args = parser.parse_args(args=[\"--data_root\", data_root, \n",
    "                               \"--checkpoint_dir\", checkpoint_dir, \n",
    "                               \"--checkpoint_path\", checkpoint_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5 # \n",
    "syncnet_mel_step_size = 16 # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(args.data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        # num_frames = (T x hop_size * fps) / sample_rate\n",
    "        start_frame_num = self.get_frame_id(start_frame)\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "\n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1)\n",
    "            vidname = self.all_videos[idx]\n",
    "\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "\n",
    "            if random.choice([True, False]):\n",
    "                y = torch.ones(1).float()\n",
    "                chosen = img_name\n",
    "            else:\n",
    "                y = torch.zeros(1).float()\n",
    "                chosen = wrong_img_name\n",
    "\n",
    "            window_fnames = self.get_window(chosen)\n",
    "            if window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = []\n",
    "            all_read = True\n",
    "            for fname in window_fnames:\n",
    "                img = cv2.imread(fname)\n",
    "                if img is None:\n",
    "                    all_read = False\n",
    "                    break\n",
    "                try:\n",
    "                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "                except Exception as e:\n",
    "                    all_read = False\n",
    "                    break\n",
    "\n",
    "                window.append(img)\n",
    "\n",
    "            if not all_read: continue\n",
    "\n",
    "            try:\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "\n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            # H x W x 3 * T\n",
    "            x = np.concatenate(window, axis=2) / 255.\n",
    "            x = x.transpose(2, 0, 1)\n",
    "            x = x[:, x.shape[1]//2:]\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "\n",
    "            return x, mel, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss = nn.BCELoss()\n",
    "\n",
    "def cosine_loss(a, v, y):\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, train_data_loader, test_data_loader, optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "    \n",
    "    while global_epoch < nepochs:\n",
    "        running_loss = 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, mel, y) in prog_bar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "\n",
    "            if global_step % hparams.syncnet_eval_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n",
    "\n",
    "            prog_bar.set_description('Loss: {}'.format(running_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n",
    "    eval_steps = 1400\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    losses = []\n",
    "    while 1:\n",
    "        for step, (x, mel, y) in enumerate(test_data_loader):\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        averaged_loss = sum(losses) / len(losses)\n",
    "        print(averaged_loss)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n",
    "\n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    global_step = checkpoint[\"global_step\"]\n",
    "    global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(os.listdir(args.data_root), train_size=0.8, shuffle=True, random_state=1)\n",
    "\n",
    "with open('C:/Users/ioann/Desktop/projects/Github/Wav2Lip/filelists/train.txt', 'w') as f:\n",
    "    for line in train:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "with open('C:/Users/ioann/Desktop/projects/Github/Wav2Lip/filelists/val.txt', 'w') as f:\n",
    "    for line in val:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloader setup\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('val')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "    train_dataset, batch_size=hparams.syncnet_batch_size, shuffle=True,\n",
    "    num_workers=hparams.num_workers)\n",
    "\n",
    "test_data_loader = data_utils.DataLoader(\n",
    "    test_dataset, batch_size=hparams.syncnet_batch_size,\n",
    "    num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = SyncNet().to(device)\n",
    "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                        lr=hparams.syncnet_lr)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "\n",
    "train(device, model, train_data_loader, test_data_loader, optimizer,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_interval=hparams.syncnet_checkpoint_interval,\n",
    "        nepochs=hparams.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/ioann/Desktop/projects/Github/Wav2Lip/data/preprocess/mvlrs_v1/00001'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirname(\"C:/Users/ioann/Desktop/projects/Github/Wav2Lip/data/preprocess/mvlrs_v1/00001/0.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/ioann/Desktop/projects/Github/Wav2Lip/data/lrs2_preprocessed'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavpath = join(\"C:/Users/ioann/Desktop/projects/Github/Wav2Lip/data/preprocess/mvlrs_v1/00001\", \"audio.wav\")\n",
    "wav = audio.load_wav(wavpath, hparams.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "import audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = audio._stft(audio.preemphasis(wav, hp.preemphasis, hp.preemphasize))\n",
    "# S = _amp_to_db(_linear_to_mel(np.abs(D))) - hp.ref_level_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mel() takes 0 positional arguments but 2 positional arguments (and 3 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m audio\u001b[38;5;241m.\u001b[39m_linear_to_mel(np\u001b[38;5;241m.\u001b[39mabs(D))\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Desktop\\projects\\Github\\Wav2Lip\\audio.py:95\u001b[0m, in \u001b[0;36m_linear_to_mel\u001b[1;34m(spectogram)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _mel_basis\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _mel_basis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     _mel_basis \u001b[38;5;241m=\u001b[39m _build_mel_basis()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(_mel_basis, spectogram)\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Desktop\\projects\\Github\\Wav2Lip\\audio.py:100\u001b[0m, in \u001b[0;36m_build_mel_basis\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_mel_basis\u001b[39m():\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m hp\u001b[38;5;241m.\u001b[39mfmax \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39msample_rate \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m librosa\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mmel(hp\u001b[38;5;241m.\u001b[39msample_rate, hp\u001b[38;5;241m.\u001b[39mn_fft, n_mels\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mnum_mels,\n\u001b[0;32m    101\u001b[0m                                fmin\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mfmin, fmax\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mfmax)\n",
      "\u001b[1;31mTypeError\u001b[0m: mel() takes 0 positional arguments but 2 positional arguments (and 3 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "audio._linear_to_mel(np.abs(D)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mel() takes 0 positional arguments but 2 positional arguments (and 3 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m librosa\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mmel(hp\u001b[38;5;241m.\u001b[39msample_rate, hp\u001b[38;5;241m.\u001b[39mn_fft, n_mels\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mnum_mels,\n\u001b[0;32m      3\u001b[0m                                fmin\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mfmin, fmax\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mfmax)\n",
      "\u001b[1;31mTypeError\u001b[0m: mel() takes 0 positional arguments but 2 positional arguments (and 3 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels,\n",
    "                               fmin=hp.fmin, fmax=hp.fmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7600"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.fmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mel() takes 0 positional arguments but 2 positional arguments (and 3 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m orig_mel \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39mmelspectrogram(wav)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Desktop\\projects\\Github\\Wav2Lip\\audio.py:47\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[1;34m(wav)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmelspectrogram\u001b[39m(wav):\n\u001b[0;32m     46\u001b[0m     D \u001b[38;5;241m=\u001b[39m _stft(preemphasis(wav, hp\u001b[38;5;241m.\u001b[39mpreemphasis, hp\u001b[38;5;241m.\u001b[39mpreemphasize))\n\u001b[1;32m---> 47\u001b[0m     S \u001b[38;5;241m=\u001b[39m _amp_to_db(_linear_to_mel(np\u001b[38;5;241m.\u001b[39mabs(D))) \u001b[38;5;241m-\u001b[39m hp\u001b[38;5;241m.\u001b[39mref_level_db\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hp\u001b[38;5;241m.\u001b[39msignal_normalization:\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _normalize(S)\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Desktop\\projects\\Github\\Wav2Lip\\audio.py:95\u001b[0m, in \u001b[0;36m_linear_to_mel\u001b[1;34m(spectogram)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _mel_basis\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _mel_basis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     _mel_basis \u001b[38;5;241m=\u001b[39m _build_mel_basis()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(_mel_basis, spectogram)\n",
      "File \u001b[1;32mc:\\Users\\ioann\\Desktop\\projects\\Github\\Wav2Lip\\audio.py:100\u001b[0m, in \u001b[0;36m_build_mel_basis\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_mel_basis\u001b[39m():\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m hp\u001b[38;5;241m.\u001b[39mfmax \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39msample_rate \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m librosa\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mmel(hp\u001b[38;5;241m.\u001b[39msample_rate, hp\u001b[38;5;241m.\u001b[39mn_fft, n_mels\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mnum_mels,\n\u001b[0;32m    101\u001b[0m                                fmin\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mfmin, fmax\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mfmax)\n",
      "\u001b[1;31mTypeError\u001b[0m: mel() takes 0 positional arguments but 2 positional arguments (and 3 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "orig_mel = audio.melspectrogram(wav).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
