{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "import audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "\n",
    "###################\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hparams import hparams as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Code to train the expert lip-sync discriminator')\n",
    "\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed LRS2 dataset\", required=True)\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)\n",
    "parser.add_argument('--checkpoint_path', help='Resumed from this checkpoint', default=None, type=str)\n",
    "\n",
    "data_root = \"C:\\\\Users\\\\ioann\\\\Desktop\\\\projects\\\\Github\\\\Wav2Lip\\\\data\\\\lrs2_preprocessed\"\n",
    "checkpoint_dir = \"C:\\\\Users\\\\ioann\\\\Desktop\\\\projects\\\\Github\\\\Wav2Lip\\\\checkpoints\\\\syncnet\"\n",
    "# checkpoint_path = None\n",
    "\n",
    "# Parse the arguments? with the values\n",
    "args = parser.parse_args(args=[\"--data_root\", data_root, \n",
    "                               \"--checkpoint_dir\", checkpoint_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5 # \n",
    "syncnet_mel_step_size = 16 # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(args.data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        # num_frames = (T x hop_size * fps) / sample_rate\n",
    "        start_frame_num = self.get_frame_id(start_frame)\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "\n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1)\n",
    "            vidname = self.all_videos[idx]\n",
    "\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "\n",
    "            if random.choice([True, False]):\n",
    "                y = torch.ones(1).float()\n",
    "                chosen = img_name\n",
    "            else:\n",
    "                y = torch.zeros(1).float()\n",
    "                chosen = wrong_img_name\n",
    "\n",
    "            window_fnames = self.get_window(chosen)\n",
    "            if window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = []\n",
    "            all_read = True\n",
    "            for fname in window_fnames:\n",
    "                img = cv2.imread(fname)\n",
    "                if img is None:\n",
    "                    all_read = False\n",
    "                    break\n",
    "                try:\n",
    "                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "                except Exception as e:\n",
    "                    all_read = False\n",
    "                    break\n",
    "\n",
    "                window.append(img)\n",
    "\n",
    "            if not all_read: continue\n",
    "\n",
    "            try:\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "\n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            # H x W x 3 * T\n",
    "            x = np.concatenate(window, axis=2) / 255.\n",
    "            x = x.transpose(2, 0, 1)\n",
    "            x = x[:, x.shape[1]//2:]\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "\n",
    "            return x, mel, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss = nn.BCELoss()\n",
    "\n",
    "def cosine_loss(a, v, y):\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n",
    "    eval_steps = 1400\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    losses = []\n",
    "    while 1:\n",
    "        for step, (x, mel, y) in enumerate(test_data_loader):\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        averaged_loss = sum(losses) / len(losses)\n",
    "        print(averaged_loss)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n",
    "\n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    global_step = checkpoint[\"global_step\"]\n",
    "    global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, train_data_loader, test_data_loader, optimizer, checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "    while global_epoch < nepochs:\n",
    "        running_loss = 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, mel, y) in prog_bar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x) # pass mel spectrogram and half-frames\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "\n",
    "            if global_step % hparams.syncnet_eval_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n",
    "\n",
    "            prog_bar.set_description('Loss: {}'.format(running_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_li, val_li = train_test_split([\"\\\\\".join(i.split(\"\\\\\")[-2:]) for i in glob(os.path.join(args.data_root, \"*/*\"))], train_size=0.8, shuffle=True, random_state=1)\n",
    "\n",
    "with open('C:/Users/ioann/Desktop/projects/Github/Wav2Lip/filelists/train.txt', 'w') as f:\n",
    "    for line in train_li:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "with open('C:/Users/ioann/Desktop/projects/Github/Wav2Lip/filelists/val.txt', 'w') as f:\n",
    "    for line in val_li:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloader setup\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('val')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "    train_dataset, batch_size=hp.syncnet_batch_size, shuffle=True,\n",
    "    num_workers=hp.num_workers)\n",
    "\n",
    "test_data_loader = data_utils.DataLoader(\n",
    "    test_dataset, batch_size=hp.syncnet_batch_size,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params 16435072\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = SyncNet().to(device)\n",
    "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                        lr=hparams.syncnet_lr)\n",
    "\n",
    "if args.checkpoint_path is not None:\n",
    "    load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\Users\\ioann\\Desktop\\projects\\Github\\Wav2Lip\\audio.py:100: FutureWarning: Pass sr=16000, n_fft=800 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels,\n",
      "Loss: 0.6945393681526184: : 1it [00:04,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: C:\\Users\\ioann\\Desktop\\projects\\Github\\Wav2Lip\\checkpoints\\checkpoint_step000000001.pth\n"
     ]
    }
   ],
   "source": [
    "train(device, \n",
    "      model, \n",
    "      train_data_loader, \n",
    "      test_data_loader, \n",
    "      optimizer,\n",
    "      args.checkpoint_dir,\n",
    "      hp.syncnet_checkpoint_interval,\n",
    "      hp.nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
